# LM Studio endpoint (adjust to your local network / localhost)
LLM_BASE_URL=http://localhost:1234/v1

# Leave empty to use the currently loaded model in LM Studio
LLM_MODEL_NAME=

# Provider and API key (defaults work with LM Studio)
# LLM_MODEL_PROVIDER=openai
# LLM_API_KEY=lm-studio

# Command execution timeout in seconds
# COMMAND_TIMEOUT=30

# Investigation stage settings
# INVESTIGATION_ENABLED=true
# MAX_CONTEXT_FILES=8
# MAX_CONTEXT_CHARS=30000

# Optional: path to MCP server configuration
# MCP_CONFIG_PATH=~/.axono/mcp.json

# Data directory for config, history, and MCP settings (default: ~/.axono)
# AXONO_DATA_DIR=~/.axono

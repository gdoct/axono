# LM Studio endpoint (adjust to your local network / localhost)
LLM_BASE_URL=http://localhost:1234/v1

# Leave empty to use the currently loaded model in LM Studio
LLM_MODEL_NAME=

# Model types (optional - for using different models for different tasks)
# instruction_model: Used for general tasks (agent, safety, coding pipeline)
# reasoning_model: Reserved for future use with reasoning-heavy tasks
#
# Resolution order:
#   1. If reasoning task and LLM_REASONING_MODEL is set, use it
#   2. If LLM_INSTRUCTION_MODEL is set, use it
#   3. If LLM_MODEL_NAME is set, use it
#   4. Empty = use the loaded model in LM Studio backend
#
# LLM_INSTRUCTION_MODEL=
# LLM_REASONING_MODEL=

# Provider and API key (defaults work with LM Studio)
# LLM_MODEL_PROVIDER=openai
# LLM_API_KEY=lm-studio

# Command execution timeout in seconds
# COMMAND_TIMEOUT=30

# Investigation stage settings
# INVESTIGATION_ENABLED=true
# MAX_CONTEXT_FILES=8
# MAX_CONTEXT_CHARS=30000

# Optional: path to MCP server configuration
# MCP_CONFIG_PATH=~/.axono/mcp.json

# Data directory for config, history, and MCP settings (default: ~/.axono)
# AXONO_DATA_DIR=~/.axono
